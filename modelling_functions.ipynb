{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/2019-11-02_reddit-data-askscience_scrubbed.csv\"\n",
    "\n",
    "# define the stop words \n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_list(file):\n",
    "    posts_df = pd.read_csv(file)\n",
    "    \n",
    "    # take only the combined column\n",
    "    posts_df = posts_df[\"combined\"]\n",
    "    # Convert the column to a list\n",
    "    corpus_text = list()\n",
    "    for row in range(posts_df.shape[0]):\n",
    "        temp = posts_df.iloc[row]\n",
    "        corpus_text.append(temp)\n",
    "\n",
    "    # Convert the list to list of lists\n",
    "    processed_corpus = list()\n",
    "\n",
    "    for text in corpus_text:\n",
    "        # tokenize it \n",
    "        tokenized_list = word_tokenize(text)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokenized_list = [w.lower() for w in tokenized_list]\n",
    "\n",
    "        # get the alphabetic words\n",
    "        words = [word for word in tokenized_list if word.isalpha()]\n",
    "\n",
    "        # get rid of stop words \n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        processed_corpus.append(words)\n",
    "    \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(corpus):\n",
    "    dictionary_reddit = corpora.Dictionary(corpus)\n",
    "    num_words = len(dictionary_reddit.keys())\n",
    "\n",
    "    # convert to a bag of words reprensentation\n",
    "    bow_corpus_reddit = [dictionary_reddit.doc2bow(text) for text in corpus]\n",
    "\n",
    "    # train the model\n",
    "    tfidf_reddit = models.TfidfModel(bow_corpus_reddit)\n",
    "\n",
    "    # similarities model\n",
    "    index_reddit = similarities.SparseMatrixSimilarity(tfidf_reddit[bow_corpus_reddit], \n",
    "                                                   num_features = num_words)\n",
    "    return index_reddit, dictionary_reddit, tfidf_reddit   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_title, index, dictionary, model):   \n",
    "    # tokenize words and convert to lower case\n",
    "    tokenized_list = word_tokenize(test_title)\n",
    "\n",
    "    # convert to lower case\n",
    "    tokenized_list = [w.lower() for w in tokenized_list]\n",
    "\n",
    "    # get the alphabetic words\n",
    "    words = [word for word in tokenized_list if word.isalpha()]\n",
    "\n",
    "    #get rid of stop words \n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # bag of words representation of the query\n",
    "    query_bow = dictionary.doc2bow(words)\n",
    "    \n",
    "    # create the similarities scores\n",
    "    sims_reddit = index[model[query_bow]]\n",
    "\n",
    "    # put scores in a dict\n",
    "    sim_scores = dict()\n",
    "    for document_number, score in sorted(enumerate(sims_reddit)):\n",
    "        sim_scores[document_number] = score\n",
    "    # sort the scores\n",
    "    sorted_sim_scores = sorted(sim_scores.items(), key=lambda kv: kv[1], reverse = True)\n",
    "    \n",
    "    \n",
    "    return sorted_sim_scores[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test using the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"scientist working oxygen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = convert_csv_to_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, dictionary, model = train_model(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(806, 0.3506022),\n",
       " (243, 0.2874382),\n",
       " (267, 0.27009386),\n",
       " (273, 0.26514146),\n",
       " (8, 0.23303704),\n",
       " (872, 0.174325),\n",
       " (945, 0.15344457),\n",
       " (50, 0.13729197),\n",
       " (638, 0.1320878),\n",
       " (129, 0.12332527)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(input_str, index, dictionary, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
